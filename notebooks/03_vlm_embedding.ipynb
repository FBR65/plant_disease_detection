{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffc0e47",
   "metadata": {},
   "source": [
    "# Vision-Language Model (VLM) Embedding\n",
    "\n",
    "Dieses Notebook demonstriert die Erstellung von Embeddings mit einem Vision-Language Model für die Pflanzenkrankheitserkennung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Pfade definieren\n",
    "DATA_PATH = Path('../data')\n",
    "MODEL_PATH = Path('../models/vlm_embedder')\n",
    "\n",
    "# CLIP-Modell laden\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7a335",
   "metadata": {},
   "source": [
    "## Embedding-Funktionen definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df9ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_embedding(image_path, text_description=None):\n",
    "    \"\"\"Erstellt Embeddings für ein Bild mit optionalem Text\"\"\"\n",
    "    # Bild laden\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Nur Bild-Embedding\n",
    "    if text_description is None:\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        return image_features.numpy().squeeze()\n",
    "    \n",
    "    # Multimodales Embedding (Bild + Text)\n",
    "    inputs = processor(text=text_description, images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.image_embeds\n",
    "        text_features = outputs.text_embeds\n",
    "    \n",
    "    return {\n",
    "        'image_embedding': image_features.numpy().squeeze(),\n",
    "        'text_embedding': text_features.numpy().squeeze(),\n",
    "        'combined_embedding': (image_features + text_features).numpy().squeeze() / 2\n",
    "    }\n",
    "\n",
    "# Textbeschreibungen für verschiedene Kategorien\n",
    "category_descriptions = {\n",
    "    'healthy': 'healthy green plant leaf without disease',\n",
    "    'disease_A': 'plant leaf with disease A symptoms',\n",
    "    'disease_B': 'plant leaf with disease B symptoms'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8876e9",
   "metadata": {},
   "source": [
    "## Embeddings für Datensatz erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_embeddings():\n",
    "    \"\"\"Erstellt Embeddings für alle Bilder im Datensatz\"\"\"\n",
    "    embeddings_data = []\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        for category in ['healthy', 'disease_A', 'disease_B']:\n",
    "            category_path = DATA_PATH / 'raw' / split / category\n",
    "            if category_path.exists():\n",
    "                for image_file in category_path.glob('*.jpg'):\n",
    "                    try:\n",
    "                        # Embedding erstellen\n",
    "                        embedding_result = create_image_embedding(\n",
    "                            image_file, \n",
    "                            category_descriptions[category]\n",
    "                        )\n",
    "                        \n",
    "                        # Metadaten sammeln\n",
    "                        embeddings_data.append({\n",
    "                            'image_path': str(image_file),\n",
    "                            'category': category,\n",
    "                            'split': split,\n",
    "                            'image_embedding': embedding_result['image_embedding'].tolist(),\n",
    "                            'text_embedding': embedding_result['text_embedding'].tolist(),\n",
    "                            'combined_embedding': embedding_result['combined_embedding'].tolist()\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"Processed: {image_file.name}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {image_file}: {e}\")\n",
    "    \n",
    "    return embeddings_data\n",
    "\n",
    "# Embeddings erstellen (auskommentiert für Demo)\n",
    "# embeddings_data = process_dataset_embeddings()\n",
    "# \n",
    "# # Embeddings speichern\n",
    "# with open('../data/processed/embeddings.json', 'w') as f:\n",
    "#     json.dump(embeddings_data, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
